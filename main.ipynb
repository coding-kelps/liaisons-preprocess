{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liaisons-preprocess\n",
    "\n",
    "This notebook outlines the preprocessing steps undertaken to generate datasets used for testing the [liaisons](https://github.com/coding-kelps/liaisons) client and [benchmarking open-source models in predicting argument relations](https://github.com/coding-kelps/liaisons-experiments).\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "For this preprocessing task, we utilized IBM's \"Claim Stance Dataset\" as the source. This dataset comprises 2,394 labeled claims across 55 controversial topics, collected from Wikipedia. Each claim is labeled based on its stance towards the topic, either \"PRO\" (supporting the topic) or \"CON\" (opposing the topic), making it an excellent resource for relation-based argument mining tasks.\n",
    "\n",
    "As of now, the dataset is available on [HuggingFace](https://huggingface.co/datasets/ibm/claim_stance). Further details on the original dataset can be found in the paper *Stance Classification of Context-Dependent Claims* (Bar-Haim et al., 2017).\n",
    "\n",
    "## About the Preprocessing\n",
    "\n",
    "The primary aim of this preprocessing is to create a representative sample of the dataset, thereby reducing computational costs for benchmarking and testing related software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from HuggingFace (https://huggingface.co/datasets/ibm/claim_stance)\n",
    "ds = load_dataset(\"ibm/claim_stance\", \"claim_stance\")\n",
    "\n",
    "# Extract train subset as pandas dataframe\n",
    "df = ds[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate value distribution proportions of the \"claims.stance\" column\n",
    "proportions = df['claims.stance'].value_counts(normalize=True)\n",
    "\n",
    "# Plotting the distribution\n",
    "sns.barplot(x=proportions.index, y=proportions.values)\n",
    "plt.title(\"Proportional distribution of stances over claims\")\n",
    "plt.xlabel('Stance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value distribution proportions of the \"topicTarget\" column\n",
    "proportions = df['topicTarget'].value_counts(normalize=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=proportions.index, y=proportions.values)\n",
    "plt.title('Proportional distribution of topics over claims')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Fix ticks position to avoid hazardous position\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "# Rotate labels and align to the right\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the counts of each combination of 'claims.stance' and 'topicTarget'\n",
    "combination_counts = df.groupby(['claims.stance', 'topicTarget']).size()\n",
    "\n",
    "# Calculate the number of unique combinations\n",
    "num_combinations = len(combination_counts)\n",
    "\n",
    "# Compute the number of entries per combination to get roughly 100 entries in total\n",
    "# Ensure that it's not greater than the minimum count among the combinations\n",
    "entries_per_combination = min(int(100 / num_combinations), combination_counts.min())\n",
    "\n",
    "sampled_df = df.groupby(['claims.stance', 'topicTarget'], group_keys=False).apply(\n",
    "    lambda x: x.sample(entries_per_combination, replace=True)\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value distribution proportions of the \"claims.stance\" column\n",
    "proportions = sampled_df['claims.stance'].value_counts(normalize=True)\n",
    "\n",
    "# Plotting the distribution\n",
    "sns.barplot(x=proportions.index, y=proportions.values)\n",
    "plt.title(\"Proportional distribution of stances over claims\")\n",
    "plt.xlabel('Stance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value distribution proportions of the \"topicTarget\" column\n",
    "proportions = sampled_df['topicTarget'].value_counts(normalize=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=proportions.index, y=proportions.values)\n",
    "plt.title('Proportional distribution of topics over claims')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Fix ticks position to avoid hazardous position\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "# Rotate labels and align to the right\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
    "\n",
    "# Adjust layout to ensure everything fits without overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract essentials column for argument relation prediction task\n",
    "clean_df = sampled_df.loc[:, [\"topicText\", \"claims.claimCorrectedText\", \"claims.stance\"]]\n",
    "\n",
    "# Rename column to fit to liaisons-experiments data template\n",
    "clean_df = clean_df.rename(columns={\"topicText\": \"argument_a\", \"claims.claimCorrectedText\": \"argument_b\", \"claims.stance\": \"relation\"})\n",
    "\n",
    "# Rename relation value categories\n",
    "clean_df['relation'] = clean_df['relation'].replace({\"PRO\": \"support\", \"CON\": \"attack\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "clean_df.to_csv(\"./ibm_claim_stance_binary_sample_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "- Bar-Haim, R., Bhattacharya, I., Dinuzzo, F., Saha, A., and Slonim, N. (2017). Stance classification of context dependent claims. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251â€“261."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
